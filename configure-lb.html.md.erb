---
title: Configuring load balancing for TAS for VMs
owner: Releng
---

You can configure load balancing for <%= vars.app_runtime_first %> by entering the names of your Load Balancers in the **Resource Config** pane of the <%= vars.app_runtime_abbr %> tile. This procedure varies by your IaaS and how you prepared ("paved") it for installing <%= vars.app_runtime_abbr %>. See the following section that corresponds to your use case.


## <a id="aws"></a> AWS

### <a id="aws-terraform"></a> AWS with Terraform

To set up load balancing for <%= vars.app_runtime_abbr %> on AWS using Terraform:

1. Create a file named `vm_extensions_config.yml` with the following content, depending on which release you are using:
    * **<%= vars.app_runtime_first %>**:

        ```
        ---
        product-name: cf
        resource-config:
          diego_brain:
            elb_names:
            - alb:SSH_TARGET_GROUP_1
            - alb:SSH_TARGET_GROUP_2
            additional_vm_extensions:
            - ssh-lb-security-groups
          router:
            elb_names:
            - alb:WEB_TARGET_GROUPS_1
            - alb:WEB_TARGET_GROUPS_2
            additional_vm_extensions:
            - web-lb-security-groups
          tcp_router:
            elb_names:
            - alb:TCP_TARGET_GROUP_1
            - alb:TCP_TARGET_GROUP_2
            additional_vm_extensions:
            - tcp-lb-security-groups
        ```
    * **Small Footprint <%= vars.app_runtime_abbr %>**:

          ```
          ---
          product-name: cf
          resource-config:
            control:
              elb_names:
              - alb:SSH_TARGET_GROUP_1
              - alb:SSH_TARGET_GROUP_2
              additional_vm_extensions:
              - ssh-lb-security-groups
            router:
              elb_names:
              - alb:WEB_TARGET_GROUPS_1
              - alb:WEB_TARGET_GROUPS_2
              additional_vm_extensions:
              - web-lb-security-groups
            tcp_router:
              elb_names:
              - alb:TCP_TARGET_GROUP_1
              - alb:TCP_TARGET_GROUP_2
              additional_vm_extensions:
              - tcp-lb-security-groups
          ```

1. Replace values in the file as follows:
    * `SSH_TARGET_GROUP_X`: Enter your SSH target groups. You can find these values by running:

        ```
        terraform output ssh_target_groups
        ```
    * `WEB_TARGET_GROUPS_X`: Enter your web target groups. You can find these values by running:

        ```
        terraform output web_target_groups
        ```
    * `TCP_TARGET_GROUP_X`: Enter your TCP target groups. You can find these values by running:

        ```
        terraform output tcp_target_groups
        ```

1. Apply the VM extension configuration using the `om` CLI. For more information about `om`, see the [Om](https://github.com/pivotal-cf/om) repository on GitHub.

    ```
    om -k \
      -t "OPS-MANAGER-FQDN" \
      -u "USERNAME" \
      -p "PASSWORD" \
      configure-product \
      -c vm_extensions_config.yml
    ```

    Where:
    <ul>
      <li><code>OPS-MANAGER-FQDN</code> is the URL at which you access your Ops Manager instance. This corresponds to <code>ops_manager_dns</code> in the Terraform output.</li>
      <li><code>USERNAME</code> is the user name you entered when configuring internal authentication.</li>
      <li>
        <code>PASSWORD</code> is the password you entered when configuring internal authentication.
        <p class="note"><strong>Note:</strong> If you did not configure internal authentication, you must edit this command to use a client ID and secret instead of user name and password. For more information, see <a href="https://github.com/pivotal-cf/om/tree/main/docs#authentication">Authentication</a> in the Om repository on GitHub.</p>
      </li>
    </ul>


### <a id="aws-manual"></a> AWS paved manually

To configure the Gorouter to use AWS Elastic Load Balancers:

1. Record the names of your ELBs. If you followed the procedures in [Preparing to Deploy Ops Manager on AWS](https://docs.pivotal.io/ops-manager/aws/prepare-env-manual.html), you created:
  * `<%= vars.platform_name_lc %>-ssh-elb`: An SSH load balancer. This is a Classic Load Balancer.
  * `<%= vars.platform_name_lc %>-tcp-elb`: A TCP load balancer. This is a Classic Load Balancer.
  * `<%= vars.platform_name_lc %>-web-elb`: A web load balancer. This is an Application Load Balancer.
    * `<%= vars.platform_name_lc %>-web-elb-target-group`: A target group for the web load balancer.

1. In the <%= vars.app_runtime_abbr %> tile, select **Resource Config**.

1. Enter the name of your SSH load balancer.
  1. Show the **Load Balancers** field underneath the job that handles SSH requests. This depends on the <%= vars.app_runtime_abbr %> release you are using:
      - **<%= vars.app_runtime_abbr %>**: Click the icon next to the **Diego Brain** job name to expand the row.
      - **Small Footprint <%= vars.app_runtime_abbr %>**: Click the icon next to the **Control** job name to expand the row.
  1. In the **Load Balancers** field, enter the name of your SSH load balancer: `<%= vars.platform_name_lc %>-ssh-elb`.

1. Click to expand the **Router** row, and fill in the **Load Balancers** field with a value determined by the type of load balancer you are using:
  * **Application Load Balancer**: Enter a comma-separated list of the names of the target group
    of your web load balancer, prefixed with `alb:`: `alb:<%= vars.platform_name_lc %>-web-elb-target-group`.
    The prefix indicates to Ops Manager that you entered the name of a target group.
    The prefix is required for AWS Application Load Balancers and Network Load Balancers.
  * **Classic Load Balancer**: Enter the name of the load balancer: `<%= vars.platform_name_lc %>-web-elb`.

1. If you activated TCP routing, expand the **TCP Router** row and enter the name of your TCP load balancer: `<%= vars.platform_name_lc %>-tcp-elb`.


## <a id="azure"></a> Azure

### <a id="azure-terraform"></a> Azure with Terraform

To configure the Gorouter to use Azure Load Balancers:

1. Select **Resource Config**.

1. Ensure a `Standard` VM type is selected for the **Router** VM. The <%= vars.app_runtime_abbr %> deployment fails if you select a `Basic` VM type.

1. Click the icon next to the **Router** job name to expand the row, showing a **Load Balancers** field and **INTERNET CONNECTED** check box underneath.

1. Enter the value of `web_lb_name` from your Terraform output in the **Resource Config** pane under **Load Balancers** for the **Router** job.

1. Click to expand the row for the job that handles SSH requests. This depends on the <%= vars.app_runtime_abbr %> release you are using:
    - **<%= vars.app_runtime_abbr %>**: Click the icon next to **Diego Brain**.
    - **Small Footprint <%= vars.app_runtime_abbr %>**: Click the icon next to **Control**.

1. For the SSH load balancer, enter the value of `diego_ssh_lb_name` from your Terraform output.

1. Ensure that the **INTERNET CONNECTED** checkboxes are deselected for all jobs.

1. Scale the number of instances as appropriate for your deployment.
  <p class="note"><strong>Note:</strong> For a high-availability deployment of Ops Manager on Azure, <%= vars.company_name %> recommends scaling the number of each <%= vars.app_runtime_abbr %> job to a minimum of three instances. Using three or more instances for each job creates a sufficient number of availability sets and fault domains for your deployment. For more information, see <a href="https://docs.pivotal.io/ops-manager/refarch/azure/azure_ref_arch.html">Azure Reference Architecture</a>.</p>

### <a id="azure-manual"></a> Azure paved manually

To configure the Gorouter to use Azure Load Balancers:

1. Select **Resource Config**.

1. Ensure a `Standard` VM type is selected for the **Router** VM. The <%= vars.app_runtime_abbr %> deployment fails if you select a `Basic` VM type.

1. Retrieve the name(s) of your external Azure Load Balancer (ALB) by navigating to the Azure portal,
   clicking **All resources**, and locating your **Load balancer** resource.
  <p class="note"><strong>Note:</strong> The Azure portal sometimes displays the names of resources with incorrect capitalization. Always use the Azure CLI to retrieve the correctly capitalized name of a resource. To see the list of resources, run <code>az network lb list</code>.</p>

1. In the **Resource Config** pane of the <%= vars.app_runtime_abbr %> tile, click the icon next to the **Router** job name to expand the row, showing a **Load Balancers** field and **INTERNET CONNECTED** check box underneath.

1. Enter the name of your external ALB in the field under **Load Balancers**.
   If you have multiple external ALBs this field accepts a comma-separated list.

1. Retrieve the name of your Diego SSH Load Balancer by navigating to the Azure portal, clicking **All resources**, and locating your **Load balancer** resource.

1. In the **Resource Config** pane of the <%= vars.app_runtime_abbr %> tile, expand the row for the job that handles SSH requests. This depends on the <%= vars.app_runtime_abbr %> release you are using:
    - **<%= vars.app_runtime_abbr %>**: Click the icon next to **Diego Brain**.
    - **Small Footprint <%= vars.app_runtime_abbr %>**: Click the icon next to **Control**.

1. Enter the name of the Diego SSH Load Balancer in the field under **Load Balancers**.

1. Ensure that the **INTERNET CONNECTED** checkboxes are deselected for all jobs.

1. Scale the number of instances as appropriate for your deployment.
  <p class="note"><strong>Note:</strong> For a high availability deployment of Ops Manager on Azure, <%= vars.company_name %> recommends scaling the number of each <%= vars.app_runtime_abbr %> job to a minimum of three instances. Using three or more instances for each job creates a sufficient number of availability sets and fault domains for your deployment. For more information, see <a href="https://docs.pivotal.io/ops-manager/refarch/azure/azure_ref_arch.html">Azure Reference Architecture</a>.</p>


## <a id="gcp"></a> GCP

### <a id="gcp-terraform"></a> GCP with Terraform

To configure the Gorouter to use GCP Load Balancers:

1. Select **Resource Config**.

1. Click the icon next to the **Router** job name to expand the row, showing a **Load Balancers** field and **INTERNET CONNECTED** check box underneath.

1. In the **Load Balancers** field, enter a comma-separated list consisting of the values of `ws_router_pool` and `http_lb_backend_name` from your Terraform output. For example, `tcp:<%= vars.platform_name_lc %>-cf-ws,http:<%= vars.platform_name_lc %>-httpslb`. These are the names of the TCP WebSockets and HTTP(S) Load Balancers for your deployment.
    <p class="note"><strong>Note:</strong> Do not add a space between key and value pairs in the <code>LOAD BALANCER</code> field, or it fails.</p>

1. If you activated TCP routing in the **Networking** pane of the <%= vars.app_runtime_abbr %> tile, add the value of `tcp_router_pool` from your Terraform output, prepended with `tcp:`, to the **Load Balancers** column of the **TCP Router** row. For example, `tcp:<%= vars.platform_name_lc %>-cf-tcp`.

1. Expand the row for the job that handles SSH requests. This depends on the <%= vars.app_runtime_abbr %> release you are using:
    - **<%= vars.app_runtime_abbr %>**: Click the icon next to **Diego Brain**.
    - **Small Footprint <%= vars.app_runtime_abbr %>**: Click the icon next to **Control**.

1. Under **Load Balancers** for the SSH job, enter the value of `ssh_router_pool` from your Terraform output, prepended with `tcp:`. For example, `tcp:<%= vars.platform_code %>-ssh-proxy`.

1. Verify that the **Internet Connected** check box for every job is activated. The Terraform templates do not provision a Network Address Translation (NAT) box for Internet connectivity to your VMs, so they are provided with ephemeral public IP addresses to allow the jobs to reach the Internet.
   <p class="note"><strong>Note:</strong> If you want to provision a Network Address Translation (NAT) box to provide Internet connectivity to your VMs instead of providing them with public IP addresses, deactivate the <strong>Internet Connected</strong> check boxes. For more information about using NAT in GCP, see <a href="https://cloud.google.com/compute/docs/networking">VPC network overview</a> in the GCP documentation.</p>

1. Click **Save**.

### <a id="gcp-manual"></a> GCP paved manually

To configure the Gorouter to use GCP Load Balancers:

1. Navigate to the GCP Console and select **Load balancing**.

    ![alt-text="The load balancer table shows three Load Balancers: pcf-router HTTPS, pcf-ssh TCP, and pcf-websockets TCP."](./images/gcp/config-lb.png)

    You can see the SSH load balancer, the HTTP(S) load balancer, the TCP WebSockets load balancer, and the TCP router that you created in [Preparing to Deploy Ops Manager on GCP](https://docs.vmware.com/en/VMware-Tanzu-Operations-Manager/3.0/vmware-tanzu-ops-manager/gcp-prepare-env-manual.html).

2. Record the name of your SSH load balancer and your TCP WebSockets load balancer, `<%= vars.platform_code %>-wss-logs` and `<%= vars.platform_code %>-ssh-proxy`.
3. Record the name of your SSH load balancer and your TCP WebSockets load balancer, `<%= vars.platform_code %>-wss-logs` and `<%= vars.platform_code %>-ssh-proxy`.

4. Click your HTTP(S) load balancer, `<%= vars.platform_code %>-global-<%= vars.platform_name_lc %>`.

    ![alt-text="The Load balancer details are displayed for the pcf-router and load balancer."](./images/gcp/pcf-router.png)

5. Under **Backend services**, record the name of the back end service of the HTTP(S) load balancer, `<%= vars.platform_code %>-http-lb-backend`.
6. Under **Backend services**, record the name of the back end service of the HTTP(S) load balancer, `<%= vars.platform_code %>-http-lb-backend`.

7. In the <%= vars.app_runtime_abbr %> tile, select **Resource Config**.

8. Click the icon next to the **Router** job name to expand the row, showing a **Load Balancers** field and **INTERNET CONNECTED** check box underneath.

9.  In the **LOAD BALANCERS** field, enter a comma-separated list consisting of the name of your TCP WebSockets load balancer and the name of your HTTP(S) load balancer back end with the protocol prepended. For example, `tcp:<%= vars.platform_code %>-wss-logs,http:<%= vars.platform_code %>-http-lb-backend`.
10. In the **Load Balancers** field, enter  consisting of the name of your TCP WebSockets load balancer and the name of your HTTP(S) load balancer back end with the protocol prepended. For example, `tcp:<%= vars.platform_code %>-wss-logs,http:<%= vars.platform_code %>-http-lb-backend`.
    <p class="note"><strong>Note:</strong> Do not add a space between key and value pairs in the <code>LOAD BALANCER</code> field, or it fails.</p>

11. If you activated TCP routing in the **Networking** pane of the <%= vars.app_runtime_abbr %> tile and set up the TCP load balancer in GCP, add the name of your TCP load balancer, prepended with `tcp:`, to the **Load Balancers** column of the **TCP Router** row. For example, `tcp:<%= vars.platform_name_lc %>-tcp-router`.

12. Expand the row for the job that handles SSH requests. This depends on the <%= vars.app_runtime_abbr %> release you are using:
    - **<%= vars.app_runtime_abbr %>**: Click the icon next to **Diego Brain**.
    - **Small Footprint <%= vars.app_runtime_abbr %>**: Click the icon next to **Control**.

13. Under **Load Balancers** for the SSH job, enter the value of `ssh_router_pool` from your Terraform output, prepended with `tcp:`. For example, `tcp:<%= vars.platform_code %>-ssh-proxy`.

14. Verify that the **Internet Connected** check box for every job is disabled. When preparing your GCP environment, you provisioned a Network Address Translation (NAT) box to provide Internet connectivity to your VMs instead of providing them with public IP addresses to allow the jobs to reach the Internet.

   <p class="note"><strong>Note:</strong> If you want to provision a Network Address Translation (NAT) box to provide Internet connectivity to your VMs instead of providing them with public IP addresses, disable the <strong>Internet Connected</strong> check boxes. For more information about using NAT in GCP, see <a href="https://cloud.google.com/compute/docs/networking">VPC network overview</a> in the GCP documentation.</p>

15. Click **Save**.


## <a id="vsphere"></a> vSphere with NSX-T or NSX-V

In the **Resource Config** pane of the <%= vars.app_runtime_abbr %> tile, you configure vSphere NSX-T or NSX-V load balancing and security group membership for <%= vars.app_runtime_abbr %> jobs.

To configure load balancing and security group membership for <%= vars.app_runtime_abbr %> jobs:

1. Select **Resource Config**.

1. For each <%= vars.app_runtime_abbr %> job that you want to load-balance, click the down arrow to reveal its configuration fields. The external-facing job instance groups in <%= vars.app_runtime_abbr %> are:
  - **Diego Brain** in <%= vars.app_runtime_abbr %>, or **Control** in Small Footprint <%= vars.app_runtime_abbr %>, for SSH traffic
  - **Router** for web traffic
  - **TCP Router** for TCP traffic

1. Enter a load balancer configuration for each job:
  - **NSX-T**:
      1. In the vSphere NSX Manager > **Advanced Networks & Security** > **Groups** pane, define an NSGroup to include VMs running each load-balanced job, such as `pas-ssh-group`, `pas-tcp-group`, and `pas-web-group`.
      1. In the **NS Groups** field in **Resource Config**, enter a comma-separated list of the NSGroups you defined.
      1. In the **Logical Load Balancer**: Enter a JSON-formatted structure defining a list of `server_pools` as pairs of `name` and `port` definitions.
  - **NSX-V**:
      1. **Security Groups**: Enter a comma separated list of Security Groups defined in NSX-V to include each load-balanced job, such as `pas-ssh-group`, `pas-tcp-group`, and `pas-web-group`.
      1. **Edge Load Balancers**: Enter a JSON-formatted structure listing edge Load Balancers, each defined by `edge_name`, `pool_name`, `security_group`, `port`, and `monitor_port` definitions.

1. Click **Save**.
